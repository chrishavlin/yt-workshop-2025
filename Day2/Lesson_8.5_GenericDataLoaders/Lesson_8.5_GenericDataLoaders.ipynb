{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57eac23e-70ea-4d0a-8f7a-9bb4211e0887",
   "metadata": {},
   "source": [
    "# Generic Data Loaders: or what to do when yt.load fails\n",
    "\n",
    "*No frontend for your data?*\n",
    "\n",
    "**Try a generic data loader!**\n",
    "\n",
    "*Thinking about writing a new frontend?* \n",
    "\n",
    "**Try a generic data loader!** \n",
    "\n",
    "Gridded data? Yes! \n",
    "\n",
    "Particle Data? Yes! \n",
    "\n",
    "(with some caveats)\n",
    "\n",
    "docs:\n",
    "* loader api: https://yt-project.org/doc/reference/api/yt.loaders.html\n",
    "* https://yt-project.org/doc/examining/Loading_Generic_Array_Data.html\n",
    "* https://yt-project.org/doc/examining/Loading_Generic_Particle_Data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6096fe-3c5f-4c0a-a7a4-82e04ca4cdbb",
   "metadata": {},
   "source": [
    "## structured grids:\n",
    "\n",
    "* `load_uniform_grid`\n",
    "* `load_amr_grids`\n",
    "* `load_octree`\n",
    "* `load_hdf5file`\n",
    "\n",
    "## particles\n",
    "* `load_particles`\n",
    "\n",
    "## unstructured grids (finite element meshes)\n",
    "* `load_unstructured_mesh`\n",
    "* `load_hexahedral_mesh`\n",
    "\n",
    "use some other package (`meshio`, others) to load connectivity, vertex coordinates and element data, coerce into form expected by yt. Detailed example here: https://github.com/chrishavlin/AGU2020/blob/main/notebooks/aspect_unstructured.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b0a70-b43c-4193-bbd0-481910fe1382",
   "metadata": {},
   "source": [
    "# loading SPH data with `load_particles`\n",
    "\n",
    "1. Can you fit all your particles in memory?\n",
    "2. Do you have only one SPH particle type?\n",
    "3. Do you have `density`, `mass` and `smoothing_length` fields?\n",
    "\n",
    "Then you can use `yt.load_particles` for your SPH data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcf74c-1a0d-48dd-8b07-e0618effcdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352b929-d2fb-40e8-bf5d-c5fcd6688ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import yt \n",
    "\n",
    "data = {}\n",
    "\n",
    "bbox = np.array([[np.inf, -np.inf], \n",
    "                [np.inf, -np.inf],\n",
    "                [np.inf, -np.inf]])\n",
    "n_part = int(1e4)\n",
    "\n",
    "def _get_x_y_or_z(n_particles):\n",
    "    x = np.arange(n_particles) / (n_particles ) \n",
    "    x = np.cos(x * np.pi * 2 / 10)\n",
    "    return x + np.random.random((n_particles,)) * 0.2\n",
    "    \n",
    "data['io', 'particle_position_x'] = _get_x_y_or_z(n_part)\n",
    "data['io', 'particle_position_y'] = _get_x_y_or_z(n_part)\n",
    "data['io', 'particle_position_z'] = _get_x_y_or_z(n_part)\n",
    "\n",
    "# for sph, need:\n",
    "data['io', 'density'] =  (10**np.random.normal(loc=-28, scale=2, size=n_part), 'g/cm**3')\n",
    "data['io', 'mass'] =  (10**np.random.normal(loc=38, scale=2, size=n_part), 'g')\n",
    "\n",
    "smo = np.random.normal(loc=30, scale=10, size=n_part)\n",
    "smo[smo<.5] = .5\n",
    "data['io', 'smoothing_length'] = (smo, 'kpc')\n",
    "\n",
    "# calculate a bbox\n",
    "for idim, dim in enumerate('xyz'):\n",
    "    bbox[idim][0] = min(bbox[idim][0], data['io', 'particle_position_'+dim].min())\n",
    "    bbox[idim][1] = max(bbox[idim][1], data['io', 'particle_position_'+dim].max())\n",
    "\n",
    "ds = yt.load_particles(data, length_unit='Mpc', bbox=bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397a5ba-11df-4080-b4d1-799260d70367",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.SlicePlot(ds, 'z', ('io', 'density'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357601f-a66f-4c30-a708-fbdafd4df438",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.ProjectionPlot(ds, (1., 1., 0), ('io', 'density'), width=(500, 'kpc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47e6ef-1e35-48c9-8019-162d9e579dd7",
   "metadata": {},
   "source": [
    "## `load_uniform_grid`\n",
    "\n",
    "\n",
    "1. Do you have a single grid with cell-centered values?\n",
    "2. Can you fit at least one field in memory?\n",
    "3. Is your grid regular (uniform or stretched)?\n",
    "\n",
    "uniform grid: $\\Delta{x}, \\Delta{y}, \\Delta{z}$\n",
    "\n",
    "![](solutions/figures/uniform_grid.png)\n",
    "\n",
    "or \n",
    "\n",
    "stretched grid: $\\Delta{x}(x), \\Delta{y}(y), \\Delta{z}(z)$:\n",
    "\n",
    "![](solutions/figures/stretched_grid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ee752-ae26-47c7-b4f0-b20773032b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ed653-18d3-4228-bf18-fd627f33351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt \n",
    "import numpy as np\n",
    "shp = (64, 64, 64)\n",
    "\n",
    "cell_widths = [\n",
    "    np.random.normal(loc=10, scale = 5, size=shp[0]),\n",
    "    np.random.normal(loc=10, scale = 5, size=shp[1]),\n",
    "    np.random.normal(loc=10, scale = 5, size=shp[2]),\n",
    "]\n",
    "\n",
    "# make them sum to 1\n",
    "for dim, cw in enumerate(cell_widths):\n",
    "    cw[cw<.1] = .1  # make sure we have > 0\n",
    "    cell_widths[dim] = cw / np.sum(cw)\n",
    "    \n",
    "    \n",
    "data = {\n",
    "    'field1': np.random.random(shp),\n",
    "}\n",
    "ds = yt.load_uniform_grid(data, shp, cell_widths=cell_widths)\n",
    "\n",
    "slc = yt.SlicePlot(ds, 'x', ('stream', 'field1'))\n",
    "slc.annotate_cell_edges()\n",
    "slc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887707c0-1606-4907-b73f-64ec17e64483",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nproc = yt.load_uniform_grid(data, shp, cell_widths=cell_widths, nprocs=8)\n",
    "\n",
    "print(f\"there are this many grids {ds_nproc.index.num_grids}\")\n",
    "\n",
    "slc = yt.SlicePlot(ds_nproc, 'z', ('stream', 'field1'))\n",
    "slc.annotate_grids()\n",
    "slc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03161a-1cba-4f78-8423-9bda401b2827",
   "metadata": {},
   "source": [
    "multiple grids, but base data is still in memory :( "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed0fb4-f0ae-4c5b-82af-d3ba26d0e7d6",
   "metadata": {},
   "source": [
    "## Loading from functions\n",
    "\n",
    "If you read the whole docstring for `load_uniform_grids`, maybe you caught the entry for the `data` argument:\n",
    "\n",
    "```\n",
    "data : dict\n",
    "    This is a dict of numpy arrays, (numpy array, unit spec) tuples.\n",
    "    Functions may also be supplied in place of numpy arrays as long as the\n",
    "    subsequent argument nprocs is not specified to be greater than 1.\n",
    "    Supplied functions much accepts the arguments (grid_object, field_name)\n",
    "    and return numpy arrays.  The keys to the dict are the field names.\n",
    "```\n",
    "\n",
    "\n",
    "This bit: **`Functions may also be supplied in place of numpy arrays`** is powerful! \n",
    "\n",
    "\n",
    "What's that mean?\n",
    "\n",
    "\n",
    "Instead of \n",
    "\n",
    "```python\n",
    "data = {\n",
    "    'field1': np.random.random(shp),\n",
    "}\n",
    "``` \n",
    "\n",
    "you can do supply a function and do **anything you want**  (almost):\n",
    "\n",
    "```python\n",
    "data = {\n",
    "    'field1': read_data_from_a_file_or_something('field1'),\n",
    "}\n",
    "```\n",
    "\n",
    "as long as `read_data_from_a_file_or_something` conforms to some rules... \n",
    "\n",
    "[yt_xarray](https://yt-xarray.readthedocs.io/) in 7 lines:\n",
    "\n",
    "```python\n",
    "\n",
    "import xarray as xr \n",
    "import yt \n",
    "\n",
    "ds_xr = open_dataset(....) \n",
    "\n",
    "def read_data_from_a_file_or_something(grid_object, field_tuple):\n",
    "    return interpolate_xr_ds_to_grid_object(grid_object, field_tuple)\n",
    "\n",
    "yt_ds = yt.load_uniform_grid({'field1': read_data_from_a_file_or_something}, nprocs = 8)\n",
    "\n",
    "```\n",
    "\n",
    "where `grid_object` is a grid:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded3d6e-44d7-4bbb-98c6-f5afd21dd8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nproc.index.grids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6196f90-e4f8-4086-9391-bd8071ea4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nproc.index.grids[0].LeftEdge, ds_nproc.index.grids[0].RightEdge, ds_nproc.index.grids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9757a8d-044b-45ae-b0d6-d45cd34f2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "g0 = ds.index.grids[0]\n",
    "g0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d6b9c-96c0-4001-8e50-78048b9541af",
   "metadata": {},
   "source": [
    "from which we can extract the extent and shape of that grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489408c-9e0c-40ea-b12b-99c2e5b3239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g0.LeftEdge, g0.RightEdge, g0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664cb127-bfe8-43ce-8fcb-fcec16263466",
   "metadata": {},
   "source": [
    "ok, so let's think about the `field_tuple` argument a bit too. This will be actually be a tuple of field type and field name:\n",
    "\n",
    "```python\n",
    "field_tuple = (field_type, actual_field_name)\n",
    "```\n",
    "\n",
    "there are some slightly finicky behaviors related to field types and the generic data loaders... most will end up with a field type called `'stream'`, and due to **reasons**, we can't use the raw field types from our primary dataset. So what we'll do in our function is replace the incoming `'stream'` fieldtype with `'enzo'` in our function:\n",
    "\n",
    "\n",
    "```python\n",
    "def load_field_from_ag(grid_object, field_tuple):\n",
    "\n",
    "    # get the extent and shape of the grid_object\n",
    "    le = # left edge of the grid object\n",
    "    re = # right edge of the grid object\n",
    "    shape = # shape of the grid object \n",
    "\n",
    "    # construct an abritrary grid on the **full** dataset\n",
    "    ag = ds.arbitrary_grid(le, re, shape)\n",
    "\n",
    "    # evaluate the field on the arbitrary grid of the full dataset \n",
    "    ds_field_tuple = ('enzo', field_tuple[1])\n",
    "    fixed_res_array = ag[ds_field_tuple].d  # make it a plain numpy array with the .d\n",
    "    return fixed_res_array\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c620c8-0dfc-4a50-807c-682fc4017be7",
   "metadata": {},
   "source": [
    "Ooooook, we are ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8864c2-c5f4-4799-9d5d-91727e7050b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_field_from_ag(grid_object, field_name):\n",
    "    # get the extent and shape of the grid_object\n",
    "    le = grid_object.LeftEdge  # left edge of the grid object\n",
    "    re = grid_object.RightEdge # right edge of the grid object \n",
    "    shape = grid_object.shape # shape of the grid object \n",
    "\n",
    "    # construct an abritrary grid on the **full** dataset    \n",
    "    ag = ds.arbitrary_grid(le, re, shape)\n",
    "\n",
    "    # evaluate the field on the arbitrary grid of the full dataset \n",
    "    # full_ds_field_name = field_name_map[field_name[1]]  # annoying.    \n",
    "    full_ds_field_name = ('enzo', field_name[1]) # replace     \n",
    "    fixed_res_array = ag[full_ds_field_name].d  # make it a plain numpy array with the .d\n",
    "    return fixed_res_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dac856-f72d-44aa-b96f-5e0c06777441",
   "metadata": {},
   "source": [
    "and let's build our field dictionary, keeping only the `'enzo'` field type fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb72ddc-d956-4a72-b932-339e6a1406f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for field in ds.field_list:    \n",
    "    if field[0] == 'enzo':\n",
    "        data[field[1]] = load_field_from_ag\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab16f2-e38d-4717-a87e-0c2e291fb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary_shape = (256, 256, 256)\n",
    "arbitrary_bbox = np.array([[.48, .52], \n",
    "                           [.48, .52],\n",
    "                           [.48, .52]])\n",
    "\n",
    "ds_delayed = yt.load_uniform_grid(data, arbitrary_shape, bbox=arbitrary_bbox, length_unit='Mpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e2d2f-9255-4ff2-830c-522990e8ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_delayed.field_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d0934-df9c-43ad-bf21-41f2462f5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.SlicePlot(ds_delayed, 'z', ('stream', 'Density'), origin='native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b12440a-eb25-4efe-a4ed-77a20e873fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd388e-21a4-424e-a01f-7968fc6d1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.SlicePlot(ds_delayed, 'z', ('stream', 'Density'), origin='native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5c532-4a58-463d-8889-6530b641b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.SlicePlot(ds_delayed, 'z', ('stream', 'Temperature'), origin='native')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72c970-c7a1-43c7-8b0f-7309a5ae9a70",
   "metadata": {},
   "source": [
    "Ok... but why??\n",
    "\n",
    "The `ds` reference in:\n",
    "\n",
    "```python\n",
    "def load_field_from_ag(grid_object, field_name):\n",
    "\n",
    "    # grid_object is for the wrapping dataset\n",
    "    le = ds.arr(grid_object.LeftEdge.to('kpc').d, 'kpc')\n",
    "    re = ds.arr(grid_object.RightEdge.to('kpc').d, 'kpc')\n",
    "    shape = grid_object.shape\n",
    "\n",
    "    \n",
    "    ag = ds.arbitrary_grid(le, re, shape)\n",
    "\n",
    "    print(le, re, shape)\n",
    "    \n",
    "    return ag[field_name_map[field_name[1]]].d\n",
    "```\n",
    "\n",
    "Could be anything! \n",
    "\n",
    "* handle to an open h5py file (`yt.load_hdf5_file`)\n",
    "* handle to an open xarray dataset (`yt_xarray`)\n",
    "* a dask array\n",
    "* a zarr store\n",
    "* ......\n",
    "\n",
    "`load_amr_grids` **also accepts functions** for loading, so you can, e.g., map chunks of a dask or zarr array to yt grid objects to handle larger-than-memory datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86be3c0-2e5f-4da5-9744-d622d399ef2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b47261-4852-4824-ad8b-73719af2fcff",
   "metadata": {},
   "source": [
    "## `load_amr_grids` \n",
    "\n",
    "Like `load_uniform_grid` but more flexible! \n",
    "\n",
    "Instead of `data`, you give it a list of `grids`:\n",
    "\n",
    "```python\n",
    "grid = {\n",
    "        \"left_edge\": le_i,\n",
    "        \"right_edge\": re_i,\n",
    "        \"dimensions\": sz_i,\n",
    "        \"level\": lev,\n",
    "        (\"stream\", \"density\"): levp1_noisy,\n",
    "        (\"stream\", \"lev_p1\"): levp1,\n",
    "    }\n",
    "```\n",
    "\n",
    "* `left_edge` and `right_edge`: the 3D left/right corners of the grid\n",
    "* `dimensions`: the shape of the grid\n",
    "* `level`: the refinement level\n",
    "* `(\"stream\", \"density\")` and `(\"stream\", \"lev_p1\")` some data fields so we can plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9c314-7efb-49b8-9f9e-00951c91ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our global bounding box\n",
    "bbox = np.array([[-1., 1.], \n",
    "                 [-1., 1.], \n",
    "                 [-1., 1.]])\n",
    "\n",
    "# size of the coarsest grid (level 0)\n",
    "sz_0 = np.array((64, 64, 64))\n",
    "\n",
    "# max refinement levels\n",
    "max_lev = 4\n",
    "\n",
    "# calculate box width, box center, grid spacing for level 0\n",
    "bbox_wid = bbox[:, 1] - bbox[:, 0]\n",
    "bbox_c = np.mean(bbox, axis=1)\n",
    "dd0 = bbox_wid / sz_0\n",
    "\n",
    "# iterate over grid levels\n",
    "sz_i = sz_0.copy()\n",
    "grids = []  # container for each grid dictionary\n",
    "for lev in range(max_lev):\n",
    "\n",
    "    # calculate the current grid's bounding box width\n",
    "    box_wid_factor = 2.0 * int(lev > 0) + int(lev == 0) * 1.0\n",
    "    bbox_wid = bbox_wid / box_wid_factor\n",
    "\n",
    "    # calculate the left/right edges\n",
    "    le_i = bbox_c - bbox_wid / 2.0\n",
    "    re_i = bbox_c + bbox_wid / 2.0\n",
    "\n",
    "    # find closest start/end index in lev 0 grid\n",
    "    start_i = np.round(le_i / dd0).astype(int)\n",
    "    end_i = np.round(re_i / dd0).astype(int)\n",
    "    sz_0 = end_i - start_i\n",
    "\n",
    "    # recompute for rounding errors (watch out!)\n",
    "    le_i = start_i * dd0\n",
    "    re_i = le_i + sz_0 * dd0\n",
    "\n",
    "    # the size of the current level\n",
    "    sz_i = sz_0 * 2**lev\n",
    "\n",
    "    # calculate some fields for this level\n",
    "    levp1 = np.full(sz_i, lev + 1.0)\n",
    "    levp1_noisy = levp1 + np.random.random(sz_i) - 0.5\n",
    "\n",
    "    # define the grid dictionary\n",
    "    grid = {\n",
    "        \"left_edge\": le_i,\n",
    "        \"right_edge\": re_i,\n",
    "        \"dimensions\": sz_i,\n",
    "        \"level\": lev,\n",
    "        (\"stream\", \"lev_p1_noisy\"): levp1_noisy,\n",
    "        (\"stream\", \"lev_p1\"): levp1,\n",
    "    }\n",
    "    grids.append(grid)\n",
    "\n",
    "ds = yt.load_amr_grids(\n",
    "    grids,\n",
    "    sz_0,\n",
    "    bbox=bbox,\n",
    "    length_unit='m',    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ee105-a658-406a-9245-fdfc6b0f9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "slc = yt.SlicePlot(ds, 'z', ('stream', 'lev_p1_noisy'))\n",
    "slc.set_log(('stream', 'lev_p1_noisy'), False)\n",
    "slc.set_cmap(('stream', 'lev_p1_noisy'), 'cmyt.pastel_r')\n",
    "slc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4736f2c-aef5-40f2-8af3-26b1e36e7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "slc.annotate_cell_edges(color=(1., 1., 1.), alpha= 0.8)\n",
    "slc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ec8d8-8e97-48da-a902-b0bb08722c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "slc.set_width(0.3, 'm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632176c-c4eb-46af-95f7-397523fa617c",
   "metadata": {},
   "source": [
    "### remember our loading from functions?\n",
    "\n",
    "```python\n",
    "    grid = {\n",
    "        \"left_edge\": le_i,\n",
    "        \"right_edge\": re_i,\n",
    "        \"dimensions\": sz_i,\n",
    "        \"level\": lev,\n",
    "        (\"stream\", \"lev_p1_noisy\"): levp1_noisy,\n",
    "        (\"stream\", \"lev_p1\"): levp1,\n",
    "    }\n",
    "```\n",
    "\n",
    "**COULD BE**\n",
    "\n",
    "```python\n",
    "    grid = {\n",
    "        \"left_edge\": le_i,\n",
    "        \"right_edge\": re_i,\n",
    "        \"dimensions\": sz_i,\n",
    "        \"level\": lev,\n",
    "        (\"stream\", \"lev_p1_noisy\"): load_from_file,\n",
    "        (\"stream\", \"lev_p1\"): load_from_file,\n",
    "    }\n",
    "```    \n",
    "\n",
    "where `load_from_file` is a function handle for loading a field from a file.\n",
    "\n",
    "Lazy evaluation! By grid!! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb60e5d-25ad-4724-8341-4fb35cffb8d0",
   "metadata": {},
   "source": [
    "### `load_hdf5_file`\n",
    "\n",
    "`load_amr_grids` + loading from function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b638c9-f3df-4003-b99f-574893592657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import os \n",
    "import numpy as np \n",
    "import yt\n",
    "my_file = 'hello.h5'\n",
    "if os.path.isfile(my_file):\n",
    "    os.remove(my_file)\n",
    "\n",
    "    \n",
    "with h5py.File(my_file, 'w') as ds_h5:\n",
    "    ds_h5.create_group('cooldata')\n",
    "    values = np.random.random((512, 512, 512))\n",
    "    print(values.shape)\n",
    "    ds_h5['cooldata'].create_dataset('density', data=values, dtype=np.float64)\n",
    "\n",
    "ds_h5 = yt.load_hdf5_file(my_file, root_node='cooldata')  # nchunks = \n",
    "\n",
    "\n",
    "reg = ds_h5.region(ds_h5.domain_center, \n",
    "                  ds_h5.domain_center - ds_h5.domain_width/4, \n",
    "                  ds_h5.domain_center + ds_h5.domain_width/4)\n",
    "slc = yt.SlicePlot(ds_h5, 'z', ('stream', 'density'), data_source=reg)\n",
    "slc.annotate_grids()\n",
    "slc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
